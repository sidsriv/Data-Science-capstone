q()
source("https://bioconductor.org/biocLite.R")
biocLite()
library(Biobase)
q()
source("https://bioconductor.org/biocLite.R")
biocLite("GEOquery")
install.packages(c("curl", "openssl", "XML", "RCurl", "httr"))
source("https://bioconductor.org/biocLite.R")
biocLite("GEOquery")
library(Geoquery)
source("https://bioconductor.org/biocLite.R")
biocLite("GEOquery")
library(Geoquery)
q()
library(Geoquery)
biocLite("GEOquery")
source("https://bioconductor.org/biocLite.R")
biocLite("GEOquery")
library(Geoquery)
library(GEOquery)
q()
biocLite("DSeq")
biocLite("DESeq")
library(DESeq)
library(DESeq2)
biocLite("DESeq2")
library(limma)
library(Limma)
source("https://bioconductor.org/biocLite.R")
biocLite("limma")
library(limma)
q()
source("https://bioconductor.org/biocLite.R")
biocLite("ALL")
ALL
library(ALL)
data(ALL)
ALL
fusion <- ALL$mol.biol %in% c("BCR/ABL", "NEG")
btLevels <- levels(ALL$BT)
btLevels
isB <- ALL$BT %in% btLevels[1:5]
ALLsubset <- ALL[, fusion & isB]
levels(ALLsubset$mol.biol)
ALLsubset$mol.biol
ALLsubset$mol.biol <- factor(ALLsubset$mol.biol)
library(limma)
modelMatrix <- model.matrix(~ALLsubset$mol.biol)
fit <- lmFit(ALLsubset, modelMatrix)
eFit <- eBayes(fit)
topGenes <- topTable(eFit, coef = 2, 500)
ALLsubset <- ALLsubset[as.numeric(rownames(topGenes)), ]
source("https://bioconductor.org/biocLite.R")
biocLite("MLInterfaces")
install.packages("rgl")
install.packages("devtools")
library(devtools)
install.packages("rgl")
library(rgl)
source("http://bioconductor.org/biocLite.R")
biocLite("rgl")
source("http://bioconductor.org/biocLite.R")
biocLite("rgl")
library(rgl)
library('kernlab')
data(spam)
str(spam[,1:5])
set.seed(3435)
trainIndicator = rbinom(4601,size = 1,prob = 0.5)
table(trainIndicator)
trainSpam = spam[trainIndicator = 1]
trainSpam = spam[trainIndicator == 1,]
testSpam = spam[trainIndicator == 0,]
names(trainSpam)
head(trainSpam)
table(trainSpam$type)
plot(trainSpam$capitalAve ~ trainSpam$type)
plot(log10(trainSpam$capitalAve+1) ~ trainSpam$type)
plot(log10(trainSpam[,1:4]+1))
plot(log10(trainSpam[,1:6]+1))
hcluster = hclust(dist(t(trainSpam[,1:57])))
plot(hcluster)
hclusterupdated = hclust(dist(t(log10(trainSpam[,1:57] + 1)))
)
plot(hclusterupdated)
trainSpam$numType = as.numeric(trainSpam$type) - 1
costFunction = function(x, y) sum(x != (y>0.5))
cverror = rep(NA,55)
library(boot)
for i in (1:55){}
for (i in 1:55){}
for (i in 1:55){}
for (i in 1:55){
lmFormula = reformulate(names(trainSpam)[i], response = "numType")
glmFit = glm(lmFormula, family = "binomial", data = trainSpam)
cverror[i] = cv.glm
cverror[i] = cv.glm(trainSpam, glmFit, costFunction, 2)$delta[2]
}
for i in (1:55){
for (i in 1:55){
lmFormula = reformulate(names(trainSpam)[i], response = "numType")
glmFit = glm(lmFormula, family = "binomial", data = trainSpam)
cverror[i] = cv.glm(trainSpam, glmFit, costFunction, 2)$delta[2]
}
q()
source('https://www.bioconductor.com')
source('https://www.bioconductor.com/biocLite.R')
source('https://www.bioconductor.org/biocLite.R')
biocLite('AgiMicroRna')
biocLite('Geoquery')
biocLite('GEOquery')
library(GEOquery0)
library(GEOquery)
gse <- getGEO("GSE62468")
clear
q()
source('https://www.bioconductor.org/biocLite.R')
biocLite('AnnotationHub','GenomicRanges',rtracklayer')
biocLite('AnnotationHub','GenomicRanges',rtracklayer')
biocLite('AnnotationHub','GenomicRanges','rtracklayer')
suppressMessages(library(AnnotationHub))
suppressMessages(library(GenomicRanges))
suppressMessages(library(rtracklayer))
ah <- AnnotationHub()
ah_human_CpG <- query(ah, c("CpG Islands", "hg19"))
ah_human_CpG
ah_human_CpG_data <- ah_human_CpG[["AH5086"]]
ah_human_CpG_data
summary(width(ah_human_CpG_data))
seqinfo(ah_human_CpG_data)
seqlevels(ah_human_CpG_data)
gaps(ah_human_CpG_data)
ah_human_CpG_reduce <- reduce(ah_human_CpG_data)
ah_human_CpG_reduce
autosome <- c(paste("chr", 1:22, sep=""))
split_data_by_chr <- split(ah_human_CpG_reduce, seqnames(ah_human_CpG_reduce))
autosome_CpG_data <- split_data_by_chr[autosome]
seqlevels(autosome_CpG_data)
unlist(autosome_CpG_data)
autosome_CpG_data[4]
ah_H3K4me <- query(ah, c("H3K4me3", "E003"))
ah_H3K4me_data <- ah_H3K4me[["AH29884"]]
seqinfo(ah_H3K4me_data)
seqlevels(ah_H3K4me_data)
ah_H3K4me_autosome_data <- subset(ah_H3K4me_data, seqnames %in% autosome)
# count base pairs
sum(width(unlist(ah_H3K4me_autosome_data)))
ah_H3K27me3 <- query(ah, c("H3K27me3", "narrowPeak", "E003"))
ah_H3K27me3
# retrieve data
ah_H3K27me3_data <- ah_H3K27me3[["AH29892"]]
summary(width(ah_H3K27me3_data))
seqlevels(ah_H3K27me3_data)
seqinfo(ah_H3K27me3_data)
# subset standard chrosome data
ah_H3K27me3_autosome_data <- subset(ah_H3K27me3_data, seqnames %in% autosome)
# calculate mean signalValue
ah_H3K27me3_autosome_data_mean <- mean(ah_H3K27me3_autosome_data$signalValue)
ah_H3K27me3_autosome_data_mean
bivalent_data <- intersect(unlist(ah_H3K4me_autosome_data), unlist(ah_H3K27me3_autosome_data))
sum(width(reduce(bivalent_data)))
how big a fraction (expressed as a number between 0 and 1) of the bivalent regions, overlap one or more CpG Islands?
CpG_bivalent_data <- findOverlaps(bivalent_data, unlist(autosome_CpG_data))
fraction_bi <- length(unique(queryHits(CpG_bivalent_data)))/length(bivalent_data)
fraction_bi
ov_CpG_bivalent <- intersect(bivalent_data, unlist(autosome_CpG_data))
fraction_CpG <- sum(width(reduce(ov_CpG_bivalent)))/sum(width(unlist(autosome_CpG_data)))
fraction_CpG
autosome_CpG_data
CpG_10k <- resize(unlist(autosome_CpG_data), width = 20000 + width(unlist(autosome_CpG_data)), fix = "center")
CpG_10k_bivalent <- intersect(CpG_10k, bivalent_data)
sum(width(CpG_10k_bivalent))
genome <- ah[["AH5018"]]
genome <- keepSeqlevels(genome, c("chr1", "chr2", "chr3", "chr4", "chr5", "chr6", "chr7", "chr8", "chr9", "chr10", "chr11", "chr12", "chr13", "chr14", "chr15", "chr16", "chr17", "chr18", "chr19", "chr20", "chr21", "chr22"))
genome_size <- sum(as.numeric(seqlengths(genome)))
# How big a fraction (expressed as a number between 0 and 1) of the human genome is contained in a CpG Island?
sum(as.numeric(width(unlist(autosome_CpG_data))))/genome_size
inOut = matrix(0, ncol = 2, nrow = 2)
colnames(inOut) = c("in", "out")
rownames(inOut) = c("in", "out")
inOut
inOut[1,1] = sum(width(intersect(bivalent_data, unlist(autosome_CpG_data), ignore.strand=TRUE)))
inOut[1,2] = sum(width(setdiff(bivalent_data, unlist(autosome_CpG_data), ignore.strand=TRUE)))
inOut[2,1] = sum(width(setdiff(unlist(autosome_CpG_data), bivalent_data, ignore.strand=TRUE)))
inOut[2,2] = genome_size - sum(inOut)
inOut
odd_ratio <- inOut[1,1]*inOut[2,2]/(inOut[1,2]*inOut[2,1])
odd_ratio
q()
source("http://bioconductor.org/biocLite.R")
biocLite("GeneSpring")
biocLite("GeneSpring")
q()
library(AnnotationHub)
library(Biostrings)
library(BSgenome)
library(GenomicRanges)
library(GenomicFeatures)
library(rtracklayer)
library(Rsamtools)
library(BSgenome.Hsapiens.UCSC.hg19)
Hsapiens
# count total bases on chr22
totalBase <- letterFrequency(Hsapiens$chr22, "A") +
letterFrequency(Hsapiens$chr22, "C") +
letterFrequency(Hsapiens$chr22, "G") +
letterFrequency(Hsapiens$chr22, "T")
# count GC bases on chr22
gcBase <- letterFrequency(Hsapiens$chr22, "GC")
#calculate GC ratio on chr22
gcContent <- gcBase/totalBase
gcContent
biocLite("agilp")
source("http://bioconductor.org/biocLite.R")
biocLite("agilp")
library("readMicroRnAFE")
biocLite("readMicroRnaAFE")
source("http://bioconductor.org/biocLite.R")
biocLite("agilent")
install.packages("entropy")
library('caret')
install.packages("caret")
install.packages("kernlab")
library('caret')
library('kernlab')
data(spam)
inrain <- createDataPartition(y=spam$type,p=0.75,list=FALSE)
training <- spam[intrain]
intrain <- createDataPartition(y=spam$type,p=0.75,list=FALSE)
training <- spam[intrain]
testing <- spam[-intrain]
dim(training)
training <- spam[intrain,]
testing <- spam[-intrain,]
dim(training)
set.seed(32323)
folds <- createFolds(y=spam$type,k=10,list = TRUE, returnTrain = TRUE)
sapply(folds,length)
folds[[2]][1:10]
folds[[1]][1:10]
modelfit <- train(type ~.,data = training, method = 'glm')
modelfit
warnings()
install.packages("ISLR")
data(WAGE)
data(Wage)
library(ggplot2)
library(caret)
data(Wage)
data(wage)
install.packages("ISLR")
data(wage)
data(Wage)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
adData = data.frame(diagnosis, predictors)
testIndex = createDataPartition(diagnosis, p=0.50, list=FALSE)
training = adData[-testIndex,]
testing = adData[testIndex,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p=3/4)[[1]]
training = mixtures[inTrain,]
testing = mixtures[-inTrain,]
xnames <- colnames(concrete)[1:8]
featurePlot(x=training[, xnames], y=training$CompressiveStrength, plot="pairs")
index <- seq_along(1:nrow(training))
ggplot(data=training, aes(x=index, y=CompressiveStrength)) + geom_point() +
theme_bw()
# Step-like pattern -> 4 categories
library(Hmisc)
cutCompressiveStrength <- cut2(training$CompressiveStrength, g=4)
summary(cutCompressiveStrength)
ggplot(data=training, aes(y=index, x=cutCompressiveStrength)) +
geom_boxplot() + geom_jitter(col="blue") + theme_bw()
# Another way
library(plyr)
splitOn <- cut2(training$Age, g=4)
splitOn <- mapvalues(splitOn,
from=levels(factor(splitOn)),
to=c("red", "blue", "yellow", "green"))
plot(training$CompressiveStrength, col=splitOn)
install.packages("shiny")
install.packages("manipulate")
library(manipulate)
manipulate(plot(1:x), x=slider(1,100))
setwd("~/Desktop/DS_capstone")
help(paste0)
# counted "manually" with CDM+F ;(
## Quiz 1
blogs_info <- file.info("en_US.blogs.txt")
size_MB <- (blogs_info$size)/1000000
twitter_input <- file("en_US.twitter.txt", "r")
twitter <- readLines(twitter_nput, skipNul=TRUE)
length(twitter)
close(twitter_input)
twitter_input <- file("en_US.twitter.txt", "r")
twitter <- readLines(twitter_input, skipNul=TRUE)
length(twitter)
close(twitter_input)
files <- paste0(c("en_US.blogs.txt","en_US.news.txt","en_US.twitter.txt"))
for(line in files) {
print(line)
con <- file(files,"r")
texts <- readLines(con, n=-1, skipNul=TRUE)
close(con)
print(max(sapply(texts,nchar)))
print(length(texts))
}
INPUT <- paste0(c("en_US.blogs.txt","en_US.news.txt","en_US.twitter.txt"))
for(line in INPUT) {
print(line)
con <- file(line,"r")
texts <- readLines(con, n=-1, skipNul=TRUE)
close(con)
print(max(sapply(texts,nchar)))
print(length(texts))
}
twitter_input <- "en_US.twitter.txt"
con <- file(twitter_input,"r")
tweets <- readLines(twitter_input, n=-1, skipNul=TRUE)
love <- length(grep("love",tweets))
hate <- length(grep("hate",tweets))
theThinLine <- love/hate
close(con)
twitter_input <- "en_US.twitter.txt"
con <- file(twitter_input,"r")
tweets <- readLines(twitter_input, n=-1, skipNul=TRUE)
biostatsTweet <- tweets[grep("biostats",tweets)]
close(con)
biostatsTweet
twitter_input <- "en_US.twitter.txt"
con <- file(twitter_input,"r")
tweets <- readLines(twitter_input, n=-1, skipNul=TRUE)
biostatsTweet <- tweets[grep("A computer once beat me at chess, but it was no match for me at kickboxing",tweets)]
close(con)
biostatsTweet
